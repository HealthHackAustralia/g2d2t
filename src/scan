#! /usr/bin/env python

import sys
import argparse
import csv

from SimpleNer import SimpleNer
from Tokeniser import Lexer

def scan(data, recognisers):
    results = []
    for r in recognisers:
        results.extend(r.recognise(data))
    return results

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-l', '--lexicon', metavar='LEXICON', type=argparse.FileType('r'), default=[], action='append')
    parser.add_argument('infile', metavar='INFILE', type=argparse.FileType('r'), default=[], nargs='*')
    parsed_args = parser.parse_args(sys.argv[1:])
    recognisers = []
    l = Lexer()
    for lexicon in parsed_args.lexicon:
        r = SimpleNer(lexicon.name)
        csv_reader = csv.DictReader(lexicon)
        for row in csv_reader:
            common_name = row['Common name']
            l.set_input(common_name)
            tokens = [t.value for t in l.lexer]
            r.train_term(tokens, row["DrugBank ID"])
        recognisers.append(r)

    for infile in parsed_args.infile:
        data = infile.read()
        l.set_input(data)
        input_tokens = list(l.lexer)
        results = scan(input_tokens, recognisers)
        print
        print "In file: ", infile.name
        if not results:
            print '\t<NOTHING>'
            continue
        identified_results = {}
        for terms, associated_id in results:
            s, e = terms[0].lexpos, terms[-1].lexpos+len(terms[-1].value)
            identified_results.setdefault((data[s:e], associated_id), []).append((s, e))
        for k in sorted(identified_results.keys()):
            term, id = k
            positions = identified_results[k]
            print '\t%s (%s) %s' % (term, id, " ".join("[%d, %d)" % (s, e) for s, e in positions))
